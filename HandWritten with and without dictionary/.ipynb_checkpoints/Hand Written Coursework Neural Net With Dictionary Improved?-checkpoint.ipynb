{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN - Neural nets with dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataset, (SHOULD BE ONLY RUN ONCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124150\n",
      "(124150, 784)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "##We needed to include skipDataParam otherwise dataset is too large for my system memory\n",
    "def extractHandWrittenLettersFromCSV(csv_File_Path,skipData):\n",
    "    global lettersTarget\n",
    "    pictureArray=[]\n",
    "    alphabets_mapper = {0:'a',1:'b',2:'c',3:'d',4:'e',5:'f',6:'g',7:'h',8:'i',9:'j',10:'k',11:'l',12:'m',13:'n',14:'o',15:'p',16:'q',17:'r',18:'s',19:'t',20:'u',21:'v',22:'w',23:'x',24:'y',25:'z'}\n",
    "    with open(csv_File_Path,newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|',quoting=csv.QUOTE_NONNUMERIC)\n",
    "        counter = 0\n",
    "        for row in reader:\n",
    "            if(counter==skipData):\n",
    "                digit_name=row.pop(0)\n",
    "                lettersTarget.append(alphabets_mapper[int(digit_name)])\n",
    "                pictureArray.append(np.asarray(row))\n",
    "                counter=0\n",
    "            counter=counter+1\n",
    "    return np.asarray(pictureArray)\n",
    "    \n",
    "lettersTarget=[]\n",
    "lettersArray = extractHandWrittenLettersFromCSV(\"A_ZHandwrittenCharacters.csv\",3)##Only getting every 7th letter since dataset is very large\n",
    "print(len(lettersTarget))\n",
    "print(lettersArray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Note the new images are 28x28\n",
    "## For image recognition\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "##For image analysis\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "##Library for extracting all images in a directory\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "\n",
    "## CONSTANTS\n",
    "folder_strings = 'abcdefghijklmnopqrstuvwxyz'\n",
    "smallestNumberOfElementsInEachCollection=sys.maxsize;\n",
    "\n",
    "def printArrayImage(dataImage,letterArray,index):\n",
    "    if(len(dataImage.shape)<=2):\n",
    "        overall=[]\n",
    "        temp = []\n",
    "        counter = 0\n",
    "        for x in range(len(dataImage[index])):\n",
    "            temp.append(dataImage[index][x])\n",
    "            counter=counter+1\n",
    "            if(counter==28):\n",
    "                overall.append(temp)\n",
    "                counter=0\n",
    "                temp=[]\n",
    "            \n",
    "        overall=np.asarray(overall)\n",
    "        plt.imshow(overall, cmap='Greys', vmin=0, vmax=255)\n",
    "        plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "        plt.show()\n",
    "        print(f\"Letter: {letterArray[index]}\")\n",
    "        return\n",
    "    \n",
    "    plt.imshow(dataImage[index], cmap='as_gray', vmin=0, vmax=255)\n",
    "    plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "def printImage(dataImage):\n",
    "    print(dataImage)\n",
    "#     if(len(dataImage.shape)<=2):\n",
    "#         overall=[]\n",
    "#         temp = []\n",
    "#         counter = 0\n",
    "#         for x in range(len(dataImage[index])):\n",
    "#             temp.append(dataImage[index][x])\n",
    "#             counter=counter+1\n",
    "#             if(counter==28):\n",
    "#                 overall.append(temp)\n",
    "#                 counter=0\n",
    "#                 temp=[]\n",
    "            \n",
    "#         overall=np.asarray(overall)\n",
    "#         plt.imshow(overall, cmap='Greys', vmin=0, vmax=255)\n",
    "#         plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "#         plt.show()\n",
    "#         print(f\"Letter: {letterArray[index]}\")\n",
    "#         return\n",
    "    \n",
    "    plt.imshow(dataImage, cmap='as_gray', vmin=0, vmax=255)\n",
    "    plt.show()\n",
    "\n",
    "def printAllImagesInDataSetWithPrediction(dataImage,letterArray,model,printImageToScreen=True,skip=0):\n",
    "    local=smallestNumberOfElementsInEachCollection\n",
    "    if(skip>0):\n",
    "        local=skip\n",
    "    print(len(dataImage))\n",
    "    for x in range(0,len(dataImage),local):\n",
    "        if(printImageToScreen):\n",
    "            printImage(lettersArray,lettersTarget,x)\n",
    "        print(f\"The Model predicted {model.predict(lettersArray[x].reshape(1,-1))}, actual {lettersTarget[x]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods for extracting the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting characters...\n",
      "Totally recognised 138 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 7 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeeson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:267: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/jeeson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:396: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/home/jeeson/anaconda3/lib/python3.7/site-packages/skimage/io/_io.py:48: UserWarning: `as_grey` has been deprecated in favor of `as_gray`\n",
      "  warn('`as_grey` has been deprecated in favor of `as_gray`')\n",
      "/home/jeeson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:400: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "/home/jeeson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:406: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally recognised 4 characters\n",
      "Totally recognised 8 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 12 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 11 characters\n",
      "Totally recognised 8 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 11 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 8 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 8 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 1 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 11 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 8 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 8 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 1 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 9 characters\n",
      "Totally recognised 11 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 8 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 7 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 12 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 1 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 9 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 1 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 1 characters\n",
      "Totally recognised 3 characters\n",
      "Totally recognised 6 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 10 characters\n",
      "Totally recognised 12 characters\n",
      "Totally recognised 2 characters\n",
      "Totally recognised 12 characters\n",
      "Totally recognised 4 characters\n",
      "Totally recognised 5 characters\n",
      "Totally recognised 10 characters\n",
      "ocr/testing/testData.txt\n",
      "Total Word count 138\n",
      "Total Char counter 706\n",
      "138 849\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ## For image recognition\n",
    "# import matplotlib.cm as cm\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "# ##For image analysis\n",
    "# from skimage.io import imread as mainImageRead\n",
    "# from skimage.transform import resize\n",
    "# from scipy.misc import imread, imresize, imsave\n",
    "# from skimage.morphology import label\n",
    "# from skimage.measure import regionprops\n",
    "# from skimage.transform import resize\n",
    "\n",
    "# ##Library for extracting all images in a directory\n",
    "# import glob\n",
    "# import re\n",
    "\n",
    "\n",
    "# class Extract_Letters:\n",
    "#     def extractFile(self, filename):\n",
    "#         image = imread(filename, 1)\n",
    "\n",
    "#         # apply threshold in order to make the image binary\n",
    "#         bw = (image < 120).astype(np.float)\n",
    "\n",
    "#         # remove artifacts connected to image border\n",
    "#         cleared = bw.copy()\n",
    "#         # clear_border(cleared)\n",
    "\n",
    "# #         # label image regions\n",
    "#         label_image = label(cleared, neighbors=4)\n",
    "#         borders = np.logical_xor(bw, cleared)\n",
    "#         label_image[borders] = -1\n",
    "        \n",
    "#         letters = list()\n",
    "#         order = list()\n",
    "\n",
    "#         for region in regionprops(label_image):\n",
    "#             minr, minc, maxr, maxc = region.bbox\n",
    "#             # skip small images\n",
    "#             if region.area > 40:\n",
    "# #             if maxr - minr > len(image) / 250:##175 MAKES IT IDEAL FOR DATA EXTRACTION!\n",
    "#                 rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "#                                           fill=False, edgecolor='red', linewidth=2)\n",
    "#                 order.append(region.bbox)\n",
    "\n",
    "#         # sort the detected characters left->right, top->bottom\n",
    "#         lines = list()\n",
    "#         first_in_line = ''\n",
    "#         counter = 0\n",
    "\n",
    "#         # worst case scenario there can be 1 character per line\n",
    "#         for x in range(len(order)):\n",
    "#             lines.append([])\n",
    "\n",
    "#         for character in order:\n",
    "#             if first_in_line == '':\n",
    "#                 first_in_line = character\n",
    "#                 lines[counter].append(character)\n",
    "#             elif abs(character[0] - first_in_line[0]) < (first_in_line[2] - first_in_line[0]):\n",
    "#                 lines[counter].append(character)\n",
    "#             elif abs(character[0] - first_in_line[0]) > (first_in_line[2] - first_in_line[0]):\n",
    "#                 first_in_line = character\n",
    "#                 counter += 1\n",
    "#                 lines[counter].append(character)\n",
    "\n",
    "#         for x in range(len(lines)):\n",
    "#             lines[x].sort(key=lambda tup: tup[1])\n",
    "\n",
    "#         final = list()\n",
    "#         prev_tr = 0\n",
    "#         prev_line_br = 0\n",
    "\n",
    "#         for i in range(len(lines)):\n",
    "#             for j in range(len(lines[i])):\n",
    "#                 tl_2 = lines[i][j][1]\n",
    "#                 bl_2 = lines[i][j][0]\n",
    "#                 if tl_2 > prev_tr and bl_2 > prev_line_br:\n",
    "#                     tl, tr, bl, br = lines[i][j]\n",
    "#                     letter_raw = bw[tl:bl, tr:br]\n",
    "#                     letter_norm = resize(letter_raw, (20, 20))\n",
    "#                     final.append(letter_norm)\n",
    "#                     prev_tr = lines[i][j][3]\n",
    "#                 if j == (len(lines[i]) - 1):\n",
    "#                     prev_line_br = lines[i][j][2]\n",
    "#             prev_tr = 0\n",
    "#             tl_2 = 0\n",
    "#             ##print ('Characters recognized: ' + str(len(final)))\n",
    "#         print(f\"Totally recognised {len(final)} characters\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         final = convertTestDataToDimensionOfTraining(final)\n",
    "\n",
    "#         return final\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         print(\"Extracting characters...\")\n",
    "\n",
    "\n",
    "# ################################## Extracting training data #############################################\n",
    "\n",
    "# # Global values\n",
    "# trainingDirectory = \"ocr/testing/\"\n",
    "# testingCreatedDirectory = \"ocr/beingTested/\"\n",
    "# testingFileNames=[]\n",
    "\n",
    "# #Get associated testing data\n",
    "# def getTestingDataText(pureFileName):\n",
    "#     ##Method for getting associated text \n",
    "#     associatedTextLocation = glob.glob(f\"{trainingDirectory+pureFileName}*.txt\")[0]\n",
    "#     print(associatedTextLocation)\n",
    "#     fileReader = open(associatedTextLocation, 'r')\n",
    "#     textFileArray = fileReader.readlines()\n",
    "#     fileReader.close()\n",
    "#     textFileArray=\"\".join(textFileArray).lower()##Removes new line and makes it lowercase\n",
    "#     textFileArray = re.sub('[^0-9a-zA-Z()]+', '', textFileArray) ##Replace everything that is not letternumeric\n",
    "#     textFileArray=list(textFileArray)\n",
    "#     return textFileArray\n",
    "\n",
    "# ##Generate array of elements into array\n",
    "# def generateTestingDataArray():\n",
    "#     global lettersTestTarget\n",
    "#     global testingFileNames\n",
    "#     extract = Extract_Letters()\n",
    "#     allTestingFiles = glob.glob(f\"{trainingDirectory}*.png\")\n",
    "#     finalImagesResult=[]\n",
    "#     finalTextResult=[]\n",
    "#     counter = 0\n",
    "#     for fileName in allTestingFiles:\n",
    "#         pureFileName=fileName.replace(\".png\",\"\").replace(trainingDirectory,\"\")\n",
    "#         testingText = getTestingDataText(pureFileName) ##Returns the array of strings in the associated text\n",
    "#         lettersTestTarget.append(testingText)\n",
    "#         testingFileNames.append(pureFileName)\n",
    "        \n",
    "#         extractedImages=extract.extractFile(fileName)\n",
    "#         imageNumericalArray=[]\n",
    "#         for image in extractedImages:\n",
    "#             imsave(f\"{testingCreatedDirectory}{counter}.png\", image)\n",
    "#             img = mainImageRead(f\"{testingCreatedDirectory}{counter}.png\",as_grey=True)\n",
    "#             arr = np.asarray(img)\n",
    "#             arr=arr.flatten()# Make dimension of array a singular arrary(SO normally its 28x28 array into 1d of size 400)\n",
    "#             counter=counter+1\n",
    "#             # Converts the image to numbers, then labels in the array letters\n",
    "#             imageNumericalArray.append(arr)\n",
    "#         finalImagesResult.append(np.asarray(imageNumericalArray))\n",
    "#     return np.asarray(finalImagesResult)\n",
    "\n",
    "\n",
    "# ##Function to pass into numpy array, can ignore\n",
    "# def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "#     pad_value = kwargs.get('padder', 10)\n",
    "#     vector[:pad_width[0]] = pad_value\n",
    "#     vector[-pad_width[1]:] = pad_value\n",
    "\n",
    "# ##Pads testing data to work with training dataset\n",
    "# def convertTestDataToDimensionOfTraining(data):\n",
    "#     finalResult=[]  \n",
    "#     for element in range(len(data)):\n",
    "#         finalResult.append(np.pad(data[element], 4, pad_with,padder=0))\n",
    "#     return finalResult\n",
    "            \n",
    "# lettersTestTarget=[]\n",
    "# lettersTestArray=generateTestingDataArray()\n",
    "\n",
    "## For image recognition\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "##For image analysis\n",
    "from skimage.io import imread as mainImageRead\n",
    "from skimage.transform import resize\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "from skimage.morphology import label\n",
    "from skimage.measure import regionprops\n",
    "##Custom skimage improt below\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import skimage as skimage\n",
    "import skimage.segmentation as segment\n",
    "\n",
    "##Library for extracting all images in a directory\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "class Extract_Letters:\n",
    "    def extractFile(self, image):\n",
    "        #         image = imread(filename, 1)\n",
    "\n",
    "        # apply threshold in order to make the image binary\n",
    "        bw = image < 120\n",
    "\n",
    "        # remove artifacts connected to image border\n",
    "        cleared = bw.copy()\n",
    "        segment.clear_border(cleared)\n",
    "\n",
    "#         # label image regions\n",
    "        label_image = label(cleared, neighbors=4)\n",
    "        borders = np.logical_xor(bw, cleared)\n",
    "        label_image[borders] = -1\n",
    "        \n",
    "        letters = list()\n",
    "        order = list()\n",
    "\n",
    "        for region in regionprops(label_image):\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            # skip small images\n",
    "            if region.area > 0:\n",
    "#             if maxr - minr > len(image) / 250:##175 MAKES IT IDEAL FOR DATA EXTRACTION!\n",
    "                rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                          fill=False, edgecolor='red', linewidth=2)\n",
    "                order.append(region.bbox)\n",
    "\n",
    "        # sort the detected characters left->right, top->bottom\n",
    "        lines = list()\n",
    "        first_in_line = ''\n",
    "        counter = 0\n",
    "\n",
    "        # worst case scenario there can be 1 character per line\n",
    "        for x in range(len(order)):\n",
    "            lines.append([])\n",
    "\n",
    "        for character in order:\n",
    "            if first_in_line == '':\n",
    "                first_in_line = character\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) < (first_in_line[2] - first_in_line[0]):\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) > (first_in_line[2] - first_in_line[0]):\n",
    "                first_in_line = character\n",
    "                counter += 1\n",
    "                lines[counter].append(character)\n",
    "\n",
    "        for x in range(len(lines)):\n",
    "            lines[x].sort(key=lambda tup: tup[1])\n",
    "\n",
    "        final = list()\n",
    "        prev_tr = 0\n",
    "        prev_line_br = 0\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            for j in range(len(lines[i])):\n",
    "                tl_2 = lines[i][j][1]\n",
    "                bl_2 = lines[i][j][0]\n",
    "                if tl_2 > prev_tr and bl_2 > prev_line_br:\n",
    "                    tl, tr, bl, br = lines[i][j]\n",
    "                    letter_raw = bw[tl:bl, tr:br]\n",
    "                    letter_norm = resize(letter_raw, (20, 20))\n",
    "                    final.append(letter_norm)\n",
    "                    prev_tr = lines[i][j][3]\n",
    "                if j == (len(lines[i]) - 1):\n",
    "                    prev_line_br = lines[i][j][2]\n",
    "            prev_tr = 0\n",
    "            tl_2 = 0\n",
    "            ##print ('Characters recognized: ' + str(len(final)))\n",
    "        print(f\"Totally recognised {len(final)} characters\")\n",
    "        final = convertTestDataToDimensionOfTraining(final)\n",
    "        return final\n",
    "    \n",
    "    \n",
    "    def extractFileWord(self, filename):\n",
    "        image = imread(filename, 1)\n",
    "\n",
    "        # apply threshold in order to make the image binary\n",
    "        bw = (image > 120).astype(np.float)\n",
    "\n",
    "        # remove artifacts connected to image border\n",
    "        cleared = bw.copy()\n",
    "        cleared = gaussian_filter(cleared, (0,3))\n",
    "        cleared = cleared < 1\n",
    "        segment.clear_border(cleared)\n",
    "\n",
    "        # clear_border(cleared)\n",
    "\n",
    "#         # label image regions\n",
    "        label_image = label(cleared, neighbors=4)\n",
    "        borders = np.logical_xor(bw, cleared)\n",
    "        label_image[borders] = -1\n",
    "\n",
    "        letters = list()\n",
    "        order = list()\n",
    "\n",
    "        for region in regionprops(label_image):\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            # skip small images\n",
    "#             if region.area > 40:\n",
    "            if region.area > 200:##175 MAKES IT IDEAL FOR DATA EXTRACTION!\n",
    "                rect = mpatches.Rectangle((minc, minr), (maxc - minc), (maxr - minr),\n",
    "                                          fill=False, edgecolor='red', linewidth=2)\n",
    "                order.append((minr-4, minc+4, maxr+4, maxc-4))\n",
    "\n",
    "        # sort the detected characters left->right, top->bottom\n",
    "        lines = list()\n",
    "        first_in_line = ''\n",
    "        counter = 0\n",
    "\n",
    "        # worst case scenario there can be 1 character per line\n",
    "        for x in range(len(order)):\n",
    "            lines.append([])\n",
    "\n",
    "        for character in order:\n",
    "            if first_in_line == '':\n",
    "                first_in_line = character\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) < (first_in_line[2] - first_in_line[0]):\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) > (first_in_line[2] - first_in_line[0]):\n",
    "                first_in_line = character\n",
    "                counter += 1\n",
    "                lines[counter].append(character)\n",
    "\n",
    "        for x in range(len(lines)):\n",
    "            lines[x].sort(key=lambda tup: tup[1])\n",
    "\n",
    "        final = list()\n",
    "        prev_tr = 0\n",
    "        prev_line_br = 0\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            for j in range(len(lines[i])):\n",
    "                tl_2 = lines[i][j][1]\n",
    "                bl_2 = lines[i][j][0]\n",
    "                if tl_2 > prev_tr and bl_2 > prev_line_br:\n",
    "                    tl, tr, bl, br = lines[i][j]\n",
    "                    letter_raw = bw[tl:bl, tr:br]\n",
    "                    final.append(letter_raw)\n",
    "                    prev_tr = lines[i][j][3]\n",
    "                if j == (len(lines[i]) - 1):\n",
    "                    prev_line_br = lines[i][j][2]\n",
    "            prev_tr = 0\n",
    "            tl_2 = 0\n",
    "            ##print ('Characters recognized: ' + str(len(final)))\n",
    "        print(f\"Totally recognised {len(final)} characters\")\n",
    "        return final\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Extracting characters...\")\n",
    "\n",
    "        \n",
    "##Function to pass into numpy array, can ignore\n",
    "def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "    pad_value = kwargs.get('padder', 10)\n",
    "    vector[:pad_width[0]] = pad_value\n",
    "    vector[-pad_width[1]:] = pad_value\n",
    "\n",
    "##Pads testing data to work with training dataset\n",
    "def convertTestDataToDimensionOfTraining(data):\n",
    "    finalResult=[]  \n",
    "    for element in range(len(data)):\n",
    "        finalResult.append(np.pad(data[element], 4, pad_with,padder=0))\n",
    "    return finalResult\n",
    "\n",
    "\n",
    "################################ Extracting testing data 2 ############################################\n",
    "#Step 1 : First extrac the image into words\n",
    "#Step 2: Pass the words into the character segmentation program\n",
    "#Step 3: Use the characters to then make a predidction on the data\n",
    "trainingDirectory = \"ocr/testing/\"\n",
    "testingWordDirectory = \"ocr/beingTestedWord/\"\n",
    "testingCharDirectory = \"ocr/beingTestedCharacters/\"\n",
    "\n",
    "\n",
    "\n",
    "# #Get associated testing data\n",
    "def getTestingDataText(filePath):\n",
    "    ##Method for getting associated text \n",
    "    associatedTextLocation = f\"{filePath[:len(filePath)-4]}.txt\"\n",
    "    print(associatedTextLocation)\n",
    "    fileReader = open(associatedTextLocation, 'r')\n",
    "    textFileArray = fileReader.readlines()\n",
    "    fileReader.close()\n",
    "    textFileArray=\"\".join(textFileArray).lower()##Removes new line and makes it lowercase\n",
    "    textFileArray = re.sub('[^0-9a-zA-Z() ]+', '', textFileArray) ##Replace everything that is not letternumeric\n",
    "#     textFileArray=list(textFileArray)\n",
    "    return textFileArray\n",
    "\n",
    "\n",
    "\n",
    "def getWordText():\n",
    "    global targetText\n",
    "    global targetImage\n",
    "    charCounter = 0\n",
    "    wordCounter = 0\n",
    "    allFileNames = glob.glob(f\"{trainingDirectory}*.png\")\n",
    "    extract = Extract_Letters()\n",
    "    for fileName in allFileNames:\n",
    "        allWordsResult = extract.extractFileWord(fileName);\n",
    "#         print(\"Total number of words founds were\",len(allWordsResult),\"\\n\\n Now Characters\")\n",
    "        for word in allWordsResult:\n",
    "            tempWordStore = []\n",
    "            imsave(f\"{testingWordDirectory}{wordCounter}.png\", word)\n",
    "            wordImage = mainImageRead(f\"{testingWordDirectory}{wordCounter}.png\",as_grey=True)\n",
    "            wordImage = np.asarray(wordImage)\n",
    "#             wordImage = np.pad(wordImage, 5, pad_with,padder=255)\n",
    "            imsave(f\"{testingWordDirectory}{wordCounter}.png\", wordImage)\n",
    "            allCharacters = extract.extractFile(wordImage)\n",
    "#             wordImage = wordImage.flatten()\n",
    "            wordCounter=wordCounter+1\n",
    "#             print(\"characters length in word\",len(allCharacters),\"\\n\")\n",
    "            for character in allCharacters:\n",
    "                imsave(f\"{testingCharDirectory}{charCounter}.png\", character)\n",
    "                charImage = mainImageRead(f\"{testingCharDirectory}{charCounter}.png\",as_grey=True)\n",
    "                charImage = np.asarray(charImage).flatten()\n",
    "                charCounter=charCounter+1\n",
    "                tempWordStore.append(charImage)\n",
    "            targetImage.append(tempWordStore)\n",
    "        targetText=getTestingDataText(fileName)\n",
    "    print(\"Total Word count\",wordCounter)\n",
    "    print(\"Total Char counter\",charCounter)\n",
    "\n",
    "targetImage=[]\n",
    "targetText = []\n",
    "getWordText()\n",
    "\n",
    "print(len(targetImage),len(targetText))\n",
    "\n",
    "#NOTE 718 SHOULD BE THE GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first alter the data to work with the keras and tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124150,)\n",
      "(124150, 784)\n"
     ]
    }
   ],
   "source": [
    "## Import the appropraite libraries\n",
    "import numpy as np\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "############################# Define methods for manipulating data to fit library###############################\n",
    "def letterConverter(arr,convert=\"number\"):\n",
    "    alphabetMapper={\"a\":0,\"b\":1,\"c\":2,\"d\":3,\"e\":4,\"f\":5,\"g\":6,\"h\":7,\"i\":8,\"j\":9,\"k\":10,\"l\":11,\"m\":12,\"n\":13,\"o\":14,\"p\":15,\"q\":16,\"r\":17,\"s\":18,\"t\":19,\"u\":20,\"v\":21,\"w\":22,\"x\":23,\"y\":24,\"z\":25,\"1\":26,\"2\":27,\"3\":28,\"4\":29,\"5\":30,\"6\":31,\"7\":32,\"8\":33,\"9\":34,\"(\":2,\")\":2,\"0\":14}\n",
    "    temp=[]\n",
    "    for letter in arr:\n",
    "        if(convert==\"number\"):\n",
    "            temp.append(alphabetMapper[letter])\n",
    "        else:\n",
    "            for letterAlphabet,alphabetNumber in alphabetMapper.items():\n",
    "                if(alphabetNumber==letter):\n",
    "                    temp.append(letterAlphabet)\n",
    "                    break\n",
    "    if(convert==\"number\"):\n",
    "        return np.asarray(temp)\n",
    "    return temp\n",
    "\n",
    "\n",
    "trainTarget=letterConverter(lettersTarget)##Neural net is configured to accept numeric outputs\n",
    "trainLetter=lettersArray/255##This ensures the neural net accepts values between 0 and 1\n",
    "print(trainTarget.shape)\n",
    "print(trainLetter.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the strucutre of the model, and save the strucutre of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 26\n"
     ]
    }
   ],
   "source": [
    "folder_strings = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "imageDimension=int(len(trainLetter[0]))\n",
    "sizeOfOutputLayer=len(folder_strings)\n",
    "print(imageDimension,sizeOfOutputLayer)\n",
    "\n",
    "\n",
    "#############################Defining the structure of the model #########################\n",
    "\n",
    "# ## Building the model, with  3 total layers, and 2 hidden layers with 64 neurons each,\n",
    "# ## The activation function being used is the relu function\n",
    "# # The output layer has 10 neurons as the output since 10 digits, using softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(64,activation=\"relu\",input_dim=imageDimension))##Note we only provide the input dim here since this is first layer\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(sizeOfOutputLayer,activation=\"softmax\"))##Remember 10 for number of output vlaues\n",
    "\n",
    "\n",
    "################################ Saving the model strucutre ###############################################\n",
    "\n",
    "##The loss function measure the error of the function\n",
    "## The optimizer is responsible for tryign to correct the errors present\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model, to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.7164 - accuracy: 0.8060\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.3110 - accuracy: 0.9144\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.2374 - accuracy: 0.9349\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.1992 - accuracy: 0.9458\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.1739 - accuracy: 0.9521\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.1553 - accuracy: 0.9568\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.1416 - accuracy: 0.9604\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.1296 - accuracy: 0.9639\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.1200 - accuracy: 0.9662\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.1122 - accuracy: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7faf67f23cd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Train the model\n",
    "#Note hte to_categoricl converts the number say 2 into [0,0,1,0,0,0,0,0,0,0]. Since it reflects what the final output layer should look like\n",
    "##And we have currently defined the output layer to have 10 numbers, as such the output length is that long\n",
    "model.fit(trainLetter,\n",
    "         np_utils.to_categorical(trainTarget),\n",
    "          epochs=10,#How many times the model should adjust its weights on the dataset to train on\n",
    "          batch_size=200,#Represents the number of samples per gradient update for the training\n",
    "          verbose=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for calulcating metrics :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#Remember that printmd is just but cooler looking :)\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score , f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def printConfusionMatrixToScreen(confusionMatrix):\n",
    "    print(\"As pure print\")\n",
    "    for x in range(len(confusionMatrix)):\n",
    "        print(\"[\",end=\"\")\n",
    "        for y in range(len(confusionMatrix[x])):\n",
    "            print(f\"|{confusionMatrix[x][y]}|\",end=\"\")\n",
    "        print(\"]\")\n",
    "      \n",
    "      \n",
    "def plotConfusionMatrix(prediction,target,fileName):\n",
    "    confusionMatrix=confusion_matrix(target,prediction)\n",
    "    df_cm = pd.DataFrame(confusionMatrix, index = [i for i in range(len(confusionMatrix[0]))],\n",
    "                  columns = [i for i in range(len(confusionMatrix[0]))])\n",
    "    plt.figure(figsize = (24,14))\n",
    "    plt.suptitle(f\"{fileName[0].upper()+fileName[1:]} Confusion Matrix\", fontsize=50)\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.show()\n",
    "    ##printConfusionMatrixToScreen(confusionMatrix)\n",
    "\n",
    "    \n",
    "def calculateAccuracyMetrics(prediction,target):\n",
    "    printmd(f\"**Accuracy:** {metrics.accuracy_score(target,prediction)}\\n\")\n",
    "    printmd(f\"**Precision:** {precision_score(target, prediction,average='weighted',zero_division=1)}\")\n",
    "    printmd(f\"**Recall:** {recall_score(target, prediction,average='weighted')}\")\n",
    "    printmd(f\"**F1 score:** {f1_score(target, prediction,average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Appropriate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printImage(dataImage,letterArray,index):\n",
    "    if(len(dataImage.shape)<=2):\n",
    "        overall=[]\n",
    "        temp = []\n",
    "        counter = 0\n",
    "        for x in range(len(dataImage[index])):\n",
    "            temp.append(dataImage[index][x])\n",
    "            counter=counter+1\n",
    "            if(counter==28):\n",
    "                overall.append(temp)\n",
    "                counter=0\n",
    "                temp=[]\n",
    "            \n",
    "        overall=np.asarray(overall)\n",
    "        plt.imshow(overall, cmap='Greys', vmin=0, vmax=255)\n",
    "        plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "        plt.show()\n",
    "        print(f\"Letter: {letterArray[index]}\")\n",
    "        return\n",
    "    \n",
    "    plt.imshow(dataImage[index], cmap='as_gray', vmin=0, vmax=255)\n",
    "    plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "    plt.show()\n",
    "            \n",
    "# lettersTestTarget=[]\n",
    "# lettersTestArray=generateTestingDataArray()\n",
    "\n",
    "# printImage(letterArray[0])\n",
    "# print(len(lettersTestTarget[0]))\n",
    "# printImage(lettersTestArray[0],lettersTestTarget[0],26)\n",
    "# print(lettersTestTarget[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text: 138\n",
      "Words in image: 138\n",
      "No dictionary correction we got  408  Correct\n",
      "With dictionary we got  421\n",
      "Words Dic 43  No Words Dic 17\n",
      "Length of array: 685 704\n",
      "accuracy of dic 0.6145985401459854\n",
      "accuracy of no dic 0.5795454545454546\n",
      "No Dictionary Metrics \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy:** 0.5795454545454546\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Precision:** 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recall:** 0.5795454545454546"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**F1 score:** 0.7338129496402878"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Metrics \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Accuracy:** 0.6145985401459854\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Precision:** 1.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Recall:** 0.6145985401459854"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**F1 score:** 0.7613019891500904"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "\n",
    "def countHowMany1s(arr):\n",
    "    counter = 0\n",
    "    for x in arr:\n",
    "        if(x==1):\n",
    "            counter=counter+1\n",
    "    return counter\n",
    "\n",
    "def testAgainstDictionary():\n",
    "    spell = Speller(lang='en')\n",
    "    identifiedCorrectlyDic=[] ##1 is yes otherwise\n",
    "    identifiedCorrectlyNoDic=[] ##1 is yes otherwise\n",
    "    textSplit=targetText.split(\" \")\n",
    "    wordCounterNoDic = 0\n",
    "    wordCounterWithDic = 0\n",
    "    \n",
    "    for wordIndex in range(len(targetImage)):\n",
    "        characters = np.asarray(targetImage[wordIndex])\n",
    "        characters = characters/255\n",
    "        prediction = model.predict(characters)\n",
    "        prediction = np.argmax(prediction, axis=1)\n",
    "        prediction = letterConverter(prediction,\"Letters\")\n",
    "        ##We check the correct values without dictionary\n",
    "        if(\"\".join(prediction).lower()==textSplit[wordIndex].lower()):\n",
    "            wordCounterNoDic= wordCounterNoDic +1\n",
    "        counter = 0\n",
    "        while counter<len(prediction) and counter<len(textSplit[wordIndex]):\n",
    "            if(prediction[counter].lower()==textSplit[wordIndex][counter].lower()):\n",
    "                identifiedCorrectlyNoDic.append(1)\n",
    "            else:\n",
    "                identifiedCorrectlyNoDic.append(0)\n",
    "            counter=counter+1\n",
    "        ##We now check against the dictionary corrected values\n",
    "        dictionaryCorrection = list(spell(\"\".join(prediction)))\n",
    "        if(\"\".join(dictionaryCorrection).lower()==textSplit[wordIndex].lower()):\n",
    "            wordCounterWithDic= wordCounterWithDic +1\n",
    "        counter = 0\n",
    "        while counter<len(dictionaryCorrection) and counter<len(textSplit[wordIndex]):\n",
    "            if(dictionaryCorrection[counter].lower()==textSplit[wordIndex][counter].lower()):\n",
    "                identifiedCorrectlyDic.append(1)\n",
    "            else:\n",
    "                identifiedCorrectlyDic.append(0)\n",
    "            counter=counter+1\n",
    "    print(\"No dictionary correction we got \",countHowMany1s(identifiedCorrectlyNoDic),\" Correct\")\n",
    "    print(\"With dictionary we got \",countHowMany1s(identifiedCorrectlyDic))\n",
    "    print(\"Words Dic\",wordCounterWithDic,\" No Words Dic\",wordCounterNoDic)\n",
    "    print(\"Length of array:\",len(identifiedCorrectlyDic),len(identifiedCorrectlyNoDic))\n",
    "    print(\"accuracy of dic\",countHowMany1s(identifiedCorrectlyDic)/len(identifiedCorrectlyDic))\n",
    "    print(\"accuracy of no dic\",countHowMany1s(identifiedCorrectlyNoDic)/len(identifiedCorrectlyNoDic))\n",
    "    print(\"No Dictionary Metrics \\n\")\n",
    "    calculateAccuracyMetrics(np.asarray(identifiedCorrectlyNoDic),np.full((1,len(identifiedCorrectlyNoDic)), 1)[0])\n",
    "    print(\"Dictionary Metrics \\n\")\n",
    "    calculateAccuracyMetrics(np.asarray(identifiedCorrectlyDic),np.full((1,len(identifiedCorrectlyDic)), 1)[0])\n",
    "    return (np.asarray(identifiedCorrectlyDic),np.asarray(identifiedCorrectlyNoDic))\n",
    "        \n",
    "# print(trainLetter.shape)\n",
    "# temp=np.asarray(targetImage[0])\n",
    "# temp=temp.reshape(len(targetImage[0]), 28, 28, 1).astype('float32')\n",
    "# temp =temp/255\n",
    "# prediction = model.predict(trainLetter)\n",
    "# predictions = np.argmax(prediction, axis=1)\n",
    "# print(predictions)\n",
    "print(\"Words in text:\",len(targetText.split(\" \")))\n",
    "print(\"Words in image:\",len(targetImage))\n",
    "noDictionary,dictionary=testAgainstDictionary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
