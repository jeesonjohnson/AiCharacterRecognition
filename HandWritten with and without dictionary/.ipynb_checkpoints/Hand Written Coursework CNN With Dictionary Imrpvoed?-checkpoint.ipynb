{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - Neural nets with dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataset, (SHOULD BE ONLY RUN ONCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "##We needed to include skipDataParam otherwise dataset is too large for my system memory\n",
    "def extractHandWrittenLettersFromCSV(csv_File_Path,skipData):\n",
    "    global lettersTarget\n",
    "    pictureArray=[]\n",
    "    alphabets_mapper = {0:'a',1:'b',2:'c',3:'d',4:'e',5:'f',6:'g',7:'h',8:'i',9:'j',10:'k',11:'l',12:'m',13:'n',14:'o',15:'p',16:'q',17:'r',18:'s',19:'t',20:'u',21:'v',22:'w',23:'x',24:'y',25:'z'}\n",
    "    with open(csv_File_Path,newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|',quoting=csv.QUOTE_NONNUMERIC)\n",
    "        counter = 0\n",
    "        for row in reader:\n",
    "            if(counter==skipData):\n",
    "                digit_name=row.pop(0)\n",
    "                lettersTarget.append(alphabets_mapper[int(digit_name)])\n",
    "                pictureArray.append(np.asarray(row))\n",
    "                counter=0\n",
    "            counter=counter+1\n",
    "    return np.asarray(pictureArray)\n",
    "    \n",
    "lettersTarget=[]\n",
    "lettersArray = extractHandWrittenLettersFromCSV(\"A_ZHandwrittenCharacters.csv\",3)##Only getting every 7th letter since dataset is very large\n",
    "print(len(lettersTarget))\n",
    "print(lettersArray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Note the new images are 28x28\n",
    "## For image recognition\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "##For image analysis\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "##Library for extracting all images in a directory\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "\n",
    "## CONSTANTS\n",
    "folder_strings = 'abcdefghijklmnopqrstuvwxyz'\n",
    "smallestNumberOfElementsInEachCollection=sys.maxsize;\n",
    "\n",
    "def printArrayImage(dataImage,letterArray,index):\n",
    "    if(len(dataImage.shape)<=2):\n",
    "        overall=[]\n",
    "        temp = []\n",
    "        counter = 0\n",
    "        for x in range(len(dataImage[index])):\n",
    "            temp.append(dataImage[index][x])\n",
    "            counter=counter+1\n",
    "            if(counter==28):\n",
    "                overall.append(temp)\n",
    "                counter=0\n",
    "                temp=[]\n",
    "            \n",
    "        overall=np.asarray(overall)\n",
    "        plt.imshow(overall, cmap='Greys', vmin=0, vmax=255)\n",
    "        plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "        plt.show()\n",
    "        print(f\"Letter: {letterArray[index]}\")\n",
    "        return\n",
    "    \n",
    "    plt.imshow(dataImage[index], cmap='as_gray', vmin=0, vmax=255)\n",
    "    plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "def printImage(dataImage):\n",
    "    print(dataImage)\n",
    "#     if(len(dataImage.shape)<=2):\n",
    "#         overall=[]\n",
    "#         temp = []\n",
    "#         counter = 0\n",
    "#         for x in range(len(dataImage[index])):\n",
    "#             temp.append(dataImage[index][x])\n",
    "#             counter=counter+1\n",
    "#             if(counter==28):\n",
    "#                 overall.append(temp)\n",
    "#                 counter=0\n",
    "#                 temp=[]\n",
    "            \n",
    "#         overall=np.asarray(overall)\n",
    "#         plt.imshow(overall, cmap='Greys', vmin=0, vmax=255)\n",
    "#         plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "#         plt.show()\n",
    "#         print(f\"Letter: {letterArray[index]}\")\n",
    "#         return\n",
    "    \n",
    "    plt.imshow(dataImage, cmap='as_gray', vmin=0, vmax=255)\n",
    "    plt.show()\n",
    "\n",
    "def printAllImagesInDataSetWithPrediction(dataImage,letterArray,model,printImageToScreen=True,skip=0):\n",
    "    local=smallestNumberOfElementsInEachCollection\n",
    "    if(skip>0):\n",
    "        local=skip\n",
    "    print(len(dataImage))\n",
    "    for x in range(0,len(dataImage),local):\n",
    "        if(printImageToScreen):\n",
    "            printImage(lettersArray,lettersTarget,x)\n",
    "        print(f\"The Model predicted {model.predict(lettersArray[x].reshape(1,-1))}, actual {lettersTarget[x]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods for extracting the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## For image recognition\n",
    "# import matplotlib.cm as cm\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "# ##For image analysis\n",
    "# from skimage.io import imread as mainImageRead\n",
    "# from skimage.transform import resize\n",
    "# from scipy.misc import imread, imresize, imsave\n",
    "# from skimage.morphology import label\n",
    "# from skimage.measure import regionprops\n",
    "# from skimage.transform import resize\n",
    "\n",
    "# ##Library for extracting all images in a directory\n",
    "# import glob\n",
    "# import re\n",
    "\n",
    "\n",
    "# class Extract_Letters:\n",
    "#     def extractFile(self, filename):\n",
    "#         image = imread(filename, 1)\n",
    "\n",
    "#         # apply threshold in order to make the image binary\n",
    "#         bw = (image < 120).astype(np.float)\n",
    "\n",
    "#         # remove artifacts connected to image border\n",
    "#         cleared = bw.copy()\n",
    "#         # clear_border(cleared)\n",
    "\n",
    "# #         # label image regions\n",
    "#         label_image = label(cleared, neighbors=4)\n",
    "#         borders = np.logical_xor(bw, cleared)\n",
    "#         label_image[borders] = -1\n",
    "        \n",
    "#         letters = list()\n",
    "#         order = list()\n",
    "\n",
    "#         for region in regionprops(label_image):\n",
    "#             minr, minc, maxr, maxc = region.bbox\n",
    "#             # skip small images\n",
    "#             if region.area > 40:\n",
    "# #             if maxr - minr > len(image) / 250:##175 MAKES IT IDEAL FOR DATA EXTRACTION!\n",
    "#                 rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "#                                           fill=False, edgecolor='red', linewidth=2)\n",
    "#                 order.append(region.bbox)\n",
    "\n",
    "#         # sort the detected characters left->right, top->bottom\n",
    "#         lines = list()\n",
    "#         first_in_line = ''\n",
    "#         counter = 0\n",
    "\n",
    "#         # worst case scenario there can be 1 character per line\n",
    "#         for x in range(len(order)):\n",
    "#             lines.append([])\n",
    "\n",
    "#         for character in order:\n",
    "#             if first_in_line == '':\n",
    "#                 first_in_line = character\n",
    "#                 lines[counter].append(character)\n",
    "#             elif abs(character[0] - first_in_line[0]) < (first_in_line[2] - first_in_line[0]):\n",
    "#                 lines[counter].append(character)\n",
    "#             elif abs(character[0] - first_in_line[0]) > (first_in_line[2] - first_in_line[0]):\n",
    "#                 first_in_line = character\n",
    "#                 counter += 1\n",
    "#                 lines[counter].append(character)\n",
    "\n",
    "#         for x in range(len(lines)):\n",
    "#             lines[x].sort(key=lambda tup: tup[1])\n",
    "\n",
    "#         final = list()\n",
    "#         prev_tr = 0\n",
    "#         prev_line_br = 0\n",
    "\n",
    "#         for i in range(len(lines)):\n",
    "#             for j in range(len(lines[i])):\n",
    "#                 tl_2 = lines[i][j][1]\n",
    "#                 bl_2 = lines[i][j][0]\n",
    "#                 if tl_2 > prev_tr and bl_2 > prev_line_br:\n",
    "#                     tl, tr, bl, br = lines[i][j]\n",
    "#                     letter_raw = bw[tl:bl, tr:br]\n",
    "#                     letter_norm = resize(letter_raw, (20, 20))\n",
    "#                     final.append(letter_norm)\n",
    "#                     prev_tr = lines[i][j][3]\n",
    "#                 if j == (len(lines[i]) - 1):\n",
    "#                     prev_line_br = lines[i][j][2]\n",
    "#             prev_tr = 0\n",
    "#             tl_2 = 0\n",
    "#             ##print ('Characters recognized: ' + str(len(final)))\n",
    "#         print(f\"Totally recognised {len(final)} characters\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         final = convertTestDataToDimensionOfTraining(final)\n",
    "\n",
    "#         return final\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         print(\"Extracting characters...\")\n",
    "\n",
    "\n",
    "# ################################## Extracting training data #############################################\n",
    "\n",
    "# # Global values\n",
    "# trainingDirectory = \"ocr/testing/\"\n",
    "# testingCreatedDirectory = \"ocr/beingTested/\"\n",
    "# testingFileNames=[]\n",
    "\n",
    "# #Get associated testing data\n",
    "# def getTestingDataText(pureFileName):\n",
    "#     ##Method for getting associated text \n",
    "#     associatedTextLocation = glob.glob(f\"{trainingDirectory+pureFileName}*.txt\")[0]\n",
    "#     print(associatedTextLocation)\n",
    "#     fileReader = open(associatedTextLocation, 'r')\n",
    "#     textFileArray = fileReader.readlines()\n",
    "#     fileReader.close()\n",
    "#     textFileArray=\"\".join(textFileArray).lower()##Removes new line and makes it lowercase\n",
    "#     textFileArray = re.sub('[^0-9a-zA-Z()]+', '', textFileArray) ##Replace everything that is not letternumeric\n",
    "#     textFileArray=list(textFileArray)\n",
    "#     return textFileArray\n",
    "\n",
    "# ##Generate array of elements into array\n",
    "# def generateTestingDataArray():\n",
    "#     global lettersTestTarget\n",
    "#     global testingFileNames\n",
    "#     extract = Extract_Letters()\n",
    "#     allTestingFiles = glob.glob(f\"{trainingDirectory}*.png\")\n",
    "#     finalImagesResult=[]\n",
    "#     finalTextResult=[]\n",
    "#     counter = 0\n",
    "#     for fileName in allTestingFiles:\n",
    "#         pureFileName=fileName.replace(\".png\",\"\").replace(trainingDirectory,\"\")\n",
    "#         testingText = getTestingDataText(pureFileName) ##Returns the array of strings in the associated text\n",
    "#         lettersTestTarget.append(testingText)\n",
    "#         testingFileNames.append(pureFileName)\n",
    "        \n",
    "#         extractedImages=extract.extractFile(fileName)\n",
    "#         imageNumericalArray=[]\n",
    "#         for image in extractedImages:\n",
    "#             imsave(f\"{testingCreatedDirectory}{counter}.png\", image)\n",
    "#             img = mainImageRead(f\"{testingCreatedDirectory}{counter}.png\",as_grey=True)\n",
    "#             arr = np.asarray(img)\n",
    "#             arr=arr.flatten()# Make dimension of array a singular arrary(SO normally its 28x28 array into 1d of size 400)\n",
    "#             counter=counter+1\n",
    "#             # Converts the image to numbers, then labels in the array letters\n",
    "#             imageNumericalArray.append(arr)\n",
    "#         finalImagesResult.append(np.asarray(imageNumericalArray))\n",
    "#     return np.asarray(finalImagesResult)\n",
    "\n",
    "\n",
    "# ##Function to pass into numpy array, can ignore\n",
    "# def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "#     pad_value = kwargs.get('padder', 10)\n",
    "#     vector[:pad_width[0]] = pad_value\n",
    "#     vector[-pad_width[1]:] = pad_value\n",
    "\n",
    "# ##Pads testing data to work with training dataset\n",
    "# def convertTestDataToDimensionOfTraining(data):\n",
    "#     finalResult=[]  \n",
    "#     for element in range(len(data)):\n",
    "#         finalResult.append(np.pad(data[element], 4, pad_with,padder=0))\n",
    "#     return finalResult\n",
    "            \n",
    "# lettersTestTarget=[]\n",
    "# lettersTestArray=generateTestingDataArray()\n",
    "\n",
    "## For image recognition\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "##For image analysis\n",
    "from skimage.io import imread as mainImageRead\n",
    "from skimage.transform import resize\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "from skimage.morphology import label\n",
    "from skimage.measure import regionprops\n",
    "##Custom skimage improt below\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import skimage as skimage\n",
    "import skimage.segmentation as segment\n",
    "\n",
    "##Library for extracting all images in a directory\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "class Extract_Letters:\n",
    "    def extractFile(self, image):\n",
    "        #         image = imread(filename, 1)\n",
    "\n",
    "        # apply threshold in order to make the image binary\n",
    "        bw = image < 120\n",
    "\n",
    "        # remove artifacts connected to image border\n",
    "        cleared = bw.copy()\n",
    "        segment.clear_border(cleared)\n",
    "\n",
    "#         # label image regions\n",
    "        label_image = label(cleared, neighbors=4)\n",
    "        borders = np.logical_xor(bw, cleared)\n",
    "        label_image[borders] = -1\n",
    "        \n",
    "        letters = list()\n",
    "        order = list()\n",
    "\n",
    "        for region in regionprops(label_image):\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            # skip small images\n",
    "            if region.area > 0:\n",
    "#             if maxr - minr > len(image) / 250:##175 MAKES IT IDEAL FOR DATA EXTRACTION!\n",
    "                rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                          fill=False, edgecolor='red', linewidth=2)\n",
    "                order.append(region.bbox)\n",
    "\n",
    "        # sort the detected characters left->right, top->bottom\n",
    "        lines = list()\n",
    "        first_in_line = ''\n",
    "        counter = 0\n",
    "\n",
    "        # worst case scenario there can be 1 character per line\n",
    "        for x in range(len(order)):\n",
    "            lines.append([])\n",
    "\n",
    "        for character in order:\n",
    "            if first_in_line == '':\n",
    "                first_in_line = character\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) < (first_in_line[2] - first_in_line[0]):\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) > (first_in_line[2] - first_in_line[0]):\n",
    "                first_in_line = character\n",
    "                counter += 1\n",
    "                lines[counter].append(character)\n",
    "\n",
    "        for x in range(len(lines)):\n",
    "            lines[x].sort(key=lambda tup: tup[1])\n",
    "\n",
    "        final = list()\n",
    "        prev_tr = 0\n",
    "        prev_line_br = 0\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            for j in range(len(lines[i])):\n",
    "                tl_2 = lines[i][j][1]\n",
    "                bl_2 = lines[i][j][0]\n",
    "                if tl_2 > prev_tr and bl_2 > prev_line_br:\n",
    "                    tl, tr, bl, br = lines[i][j]\n",
    "                    letter_raw = bw[tl:bl, tr:br]\n",
    "                    letter_norm = resize(letter_raw, (20, 20))\n",
    "                    final.append(letter_norm)\n",
    "                    prev_tr = lines[i][j][3]\n",
    "                if j == (len(lines[i]) - 1):\n",
    "                    prev_line_br = lines[i][j][2]\n",
    "            prev_tr = 0\n",
    "            tl_2 = 0\n",
    "            ##print ('Characters recognized: ' + str(len(final)))\n",
    "        print(f\"Totally recognised {len(final)} characters\")\n",
    "        final = convertTestDataToDimensionOfTraining(final)\n",
    "        return final\n",
    "    \n",
    "    \n",
    "    def extractFileWord(self, filename):\n",
    "        image = imread(filename, 1)\n",
    "\n",
    "        # apply threshold in order to make the image binary\n",
    "        bw = (image > 120).astype(np.float)\n",
    "\n",
    "        # remove artifacts connected to image border\n",
    "        cleared = bw.copy()\n",
    "        cleared = gaussian_filter(cleared, (0,3))\n",
    "        cleared = cleared < 1\n",
    "        segment.clear_border(cleared)\n",
    "\n",
    "        # clear_border(cleared)\n",
    "\n",
    "#         # label image regions\n",
    "        label_image = label(cleared, neighbors=4)\n",
    "        borders = np.logical_xor(bw, cleared)\n",
    "        label_image[borders] = -1\n",
    "\n",
    "        letters = list()\n",
    "        order = list()\n",
    "\n",
    "        for region in regionprops(label_image):\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            # skip small images\n",
    "#             if region.area > 40:\n",
    "            if region.area > 200:##175 MAKES IT IDEAL FOR DATA EXTRACTION!\n",
    "                rect = mpatches.Rectangle((minc, minr), (maxc - minc), (maxr - minr),\n",
    "                                          fill=False, edgecolor='red', linewidth=2)\n",
    "                order.append((minr-4, minc+4, maxr+4, maxc-4))\n",
    "\n",
    "        # sort the detected characters left->right, top->bottom\n",
    "        lines = list()\n",
    "        first_in_line = ''\n",
    "        counter = 0\n",
    "\n",
    "        # worst case scenario there can be 1 character per line\n",
    "        for x in range(len(order)):\n",
    "            lines.append([])\n",
    "\n",
    "        for character in order:\n",
    "            if first_in_line == '':\n",
    "                first_in_line = character\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) < (first_in_line[2] - first_in_line[0]):\n",
    "                lines[counter].append(character)\n",
    "            elif abs(character[0] - first_in_line[0]) > (first_in_line[2] - first_in_line[0]):\n",
    "                first_in_line = character\n",
    "                counter += 1\n",
    "                lines[counter].append(character)\n",
    "\n",
    "        for x in range(len(lines)):\n",
    "            lines[x].sort(key=lambda tup: tup[1])\n",
    "\n",
    "        final = list()\n",
    "        prev_tr = 0\n",
    "        prev_line_br = 0\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            for j in range(len(lines[i])):\n",
    "                tl_2 = lines[i][j][1]\n",
    "                bl_2 = lines[i][j][0]\n",
    "                if tl_2 > prev_tr and bl_2 > prev_line_br:\n",
    "                    tl, tr, bl, br = lines[i][j]\n",
    "                    letter_raw = bw[tl:bl, tr:br]\n",
    "                    final.append(letter_raw)\n",
    "                    prev_tr = lines[i][j][3]\n",
    "                if j == (len(lines[i]) - 1):\n",
    "                    prev_line_br = lines[i][j][2]\n",
    "            prev_tr = 0\n",
    "            tl_2 = 0\n",
    "            ##print ('Characters recognized: ' + str(len(final)))\n",
    "        print(f\"Totally recognised {len(final)} characters\")\n",
    "        return final\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Extracting characters...\")\n",
    "\n",
    "##Function to pass into numpy array, can ignore\n",
    "def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "    pad_value = kwargs.get('padder', 10)\n",
    "    vector[:pad_width[0]] = pad_value\n",
    "    vector[-pad_width[1]:] = pad_value\n",
    "\n",
    "##Pads testing data to work with training dataset\n",
    "def convertTestDataToDimensionOfTraining(data):\n",
    "    finalResult=[]  \n",
    "    for element in range(len(data)):\n",
    "        finalResult.append(np.pad(data[element], 4, pad_with,padder=0))\n",
    "    return finalResult\n",
    "\n",
    "################################ Extracting testing data 2 ############################################\n",
    "#Step 1 : First extrac the image into words\n",
    "#Step 2: Pass the words into the character segmentation program\n",
    "#Step 3: Use the characters to then make a predidction on the data\n",
    "trainingDirectory = \"ocr/testing/\"\n",
    "testingWordDirectory = \"ocr/beingTestedWord/\"\n",
    "testingCharDirectory = \"ocr/beingTestedCharacters/\"\n",
    "\n",
    "\n",
    "# #Get associated testing data\n",
    "def getTestingDataText(filePath):\n",
    "    ##Method for getting associated text \n",
    "    associatedTextLocation = f\"{filePath[:len(filePath)-4]}.txt\"\n",
    "    print(associatedTextLocation)\n",
    "    fileReader = open(associatedTextLocation, 'r')\n",
    "    textFileArray = fileReader.readlines()\n",
    "    fileReader.close()\n",
    "    textFileArray=\"\".join(textFileArray).lower()##Removes new line and makes it lowercase\n",
    "    textFileArray = re.sub('[^0-9a-zA-Z() ]+', '', textFileArray) ##Replace everything that is not letternumeric\n",
    "#     textFileArray=list(textFileArray)\n",
    "    return textFileArray\n",
    "\n",
    "\n",
    "\n",
    "def getWordText():\n",
    "    global targetText\n",
    "    global targetImage\n",
    "    charCounter = 0\n",
    "    wordCounter = 0\n",
    "    allFileNames = glob.glob(f\"{trainingDirectory}*.png\")\n",
    "    extract = Extract_Letters()\n",
    "    for fileName in allFileNames:\n",
    "        allWordsResult = extract.extractFileWord(fileName);\n",
    "#         print(\"Total number of words founds were\",len(allWordsResult),\"\\n\\n Now Characters\")\n",
    "        for word in allWordsResult:\n",
    "            tempWordStore = []\n",
    "            imsave(f\"{testingWordDirectory}{wordCounter}.png\", word)\n",
    "            wordImage = mainImageRead(f\"{testingWordDirectory}{wordCounter}.png\",as_grey=True)\n",
    "            wordImage = np.asarray(wordImage)\n",
    "#             wordImage = np.pad(wordImage, 5, pad_with,padder=255)\n",
    "            imsave(f\"{testingWordDirectory}{wordCounter}.png\", wordImage)\n",
    "            allCharacters = extract.extractFile(wordImage)\n",
    "#             wordImage = wordImage.flatten()\n",
    "            wordCounter=wordCounter+1\n",
    "#             print(\"characters length in word\",len(allCharacters),\"\\n\")\n",
    "            for character in allCharacters:\n",
    "                imsave(f\"{testingCharDirectory}{charCounter}.png\", character)\n",
    "                charImage = mainImageRead(f\"{testingCharDirectory}{charCounter}.png\",as_grey=True)\n",
    "                charImage = np.asarray(charImage).flatten()\n",
    "                charCounter=charCounter+1\n",
    "                tempWordStore.append(charImage)\n",
    "            targetImage.append(tempWordStore)\n",
    "        targetText=getTestingDataText(fileName)\n",
    "    print(\"Total Word count\",wordCounter)\n",
    "    print(\"Total Char counter\",charCounter)\n",
    "\n",
    "targetImage=[]\n",
    "targetText = []\n",
    "getWordText()\n",
    "\n",
    "print(len(targetImage),len(targetText))\n",
    "\n",
    "#NOTE 718 SHOULD BE THE GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first alter the data to work with the keras and tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import the appropraite libraries\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers.convolutional import Conv2D\n",
    "# from keras.layers.convolutional import MaxPooling2D\n",
    "# from keras.utils import to_categorical\n",
    "# from keras import backend as K\n",
    "# from keras.utils import np_utils\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ############################# Define methods for manipulating data to fit library###############################\n",
    "# def letterConverter(arr,convert=\"number\"):\n",
    "#     alphabetMapper={\"a\":0,\"b\":1,\"c\":2,\"d\":3,\"e\":4,\"f\":5,\"g\":6,\"h\":7,\"i\":8,\"j\":9,\"k\":10,\"l\":11,\"m\":12,\"n\":13,\"o\":14,\"p\":15,\"q\":16,\"r\":17,\"s\":18,\"t\":19,\"u\":20,\"v\":21,\"w\":22,\"x\":23,\"y\":24,\"z\":25,\"1\":26,\"2\":27,\"3\":28,\"4\":29,\"5\":30,\"6\":31,\"7\":32,\"8\":33,\"9\":34,\"(\":2,\")\":2,\"0\":14}\n",
    "#     overall =[]\n",
    "#     temp=[]\n",
    "#     for letter in arr:\n",
    "#         if(convert==\"number\"):\n",
    "#             if(letter == \" \"):\n",
    "#                 overall.append(temp)\n",
    "#                 temp=[]\n",
    "#             else:\n",
    "#                 temp.append(alphabetMapper[letter])\n",
    "#         else:\n",
    "#             for letterAlphabet,alphabetNumber in alphabetMapper.items():\n",
    "#                 if(letter==\" \"):\n",
    "#                     overall.append(temp)\n",
    "#                     temp=[]\n",
    "#                     break\n",
    "#                 if(alphabetNumber==letter):\n",
    "#                     temp.append(letterAlphabet)\n",
    "#                     break\n",
    "#     overall.append(temp)# To account for last word not having a space\n",
    "#     if(convert==\"number\"):\n",
    "#         return np.asarray(overall)\n",
    "#     return overall\n",
    "\n",
    "# trainTarget=letterConverter(targetText)##Neural net is configured to accept numeric outputs\n",
    "# trainLetter=[]\n",
    "# for word in targetImage:\n",
    "#     temp = np.asarray(word)\n",
    "#     temp=temp.reshape(temp.shape[0], 28, 28, 1).astype('float32')\n",
    "#     temp = temp/255\n",
    "#     trainLetter.append(temp)\n",
    "\n",
    "# print(len(trainTarget))\n",
    "# print(len(trainLetter))\n",
    "\n",
    "\n",
    "## Import the appropraite libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "############################# Define methods for manipulating data to fit library###############################\n",
    "def letterConverter(arr,convert=\"number\"):\n",
    "    alphabetMapper={\"a\":0,\"b\":1,\"c\":2,\"d\":3,\"e\":4,\"f\":5,\"g\":6,\"h\":7,\"i\":8,\"j\":9,\"k\":10,\"l\":11,\"m\":12,\"n\":13,\"o\":14,\"p\":15,\"q\":16,\"r\":17,\"s\":18,\"t\":19,\"u\":20,\"v\":21,\"w\":22,\"x\":23,\"y\":24,\"z\":25,\"1\":26,\"2\":27,\"3\":28,\"4\":29,\"5\":30,\"6\":31,\"7\":32,\"8\":33,\"9\":34,\"(\":2,\")\":2,\"0\":14}\n",
    "    temp=[]\n",
    "    for letter in arr:\n",
    "        if(convert==\"number\"):\n",
    "            temp.append(alphabetMapper[letter])\n",
    "        else:\n",
    "            for letterAlphabet,alphabetNumber in alphabetMapper.items():\n",
    "                if(alphabetNumber==letter):\n",
    "                    temp.append(letterAlphabet)\n",
    "                    break\n",
    "    if(convert==\"number\"):\n",
    "        return np.asarray(temp)\n",
    "    return temp\n",
    "\n",
    "\n",
    "trainTarget=letterConverter(lettersTarget)##Neural net is configured to accept numeric outputs\n",
    "trainLetter=lettersArray.reshape(lettersArray.shape[0], 28, 28, 1).astype('float32')\n",
    "trainLetter=trainLetter/255##This ensures the neural net accepts values between 0 and 1\n",
    "print(trainTarget.shape)\n",
    "print(trainLetter.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the strucutre of the model, and save the strucutre of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_strings = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "imageDimension=int(len(trainLetter[0][0])**2)\n",
    "imageXDimension=int(np.sqrt(imageDimension))\n",
    "sizeOfOutputLayer=len(folder_strings)\n",
    "print(imageXDimension,imageDimension,sizeOfOutputLayer)\n",
    "\n",
    "#############################Defining the structure of the model #########################\n",
    "\n",
    "# ## Building the model, with  3 total layers, and 2 hidden layers with 64 neurons each,\n",
    "# ## The activation function being used is the relu function\n",
    "# # The output layer has 10 neurons as the output since 10 digits, using softmax\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(imageXDimension, imageXDimension, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(sizeOfOutputLayer, activation='softmax'))\n",
    "\n",
    "################################ Saving the model strucutre ###############################################\n",
    "\n",
    "##The loss function measure the error of the function\n",
    "## The optimizer is responsible for tryign to correct the errors present\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model, to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train the model\n",
    "#Note hte to_categoricl converts the number say 2 into [0,0,1,0,0,0,0,0,0,0]. Since it reflects what the final output layer should look like\n",
    "##And we have currently defined the output layer to have 10 numbers, as such the output length is that long\n",
    "model.fit(trainLetter,\n",
    "         np_utils.to_categorical(trainTarget),\n",
    "          epochs=10,#How many times the model should adjust its weights on the dataset to train on\n",
    "          batch_size=200,#Represents the number of samples per gradient update for the training\n",
    "          verbose=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for calulcating metrics :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#Remember that printmd is just but cooler looking :)\n",
    "from sklearn import metrics\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score , f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def printConfusionMatrixToScreen(confusionMatrix):\n",
    "    print(\"As pure print\")\n",
    "    for x in range(len(confusionMatrix)):\n",
    "        print(\"[\",end=\"\")\n",
    "        for y in range(len(confusionMatrix[x])):\n",
    "            print(f\"|{confusionMatrix[x][y]}|\",end=\"\")\n",
    "        print(\"]\")\n",
    "      \n",
    "      \n",
    "def plotConfusionMatrix(prediction,target,fileName):\n",
    "    confusionMatrix=confusion_matrix(target,prediction)\n",
    "    df_cm = pd.DataFrame(confusionMatrix, index = [i for i in range(len(confusionMatrix[0]))],\n",
    "                  columns = [i for i in range(len(confusionMatrix[0]))])\n",
    "    plt.figure(figsize = (24,14))\n",
    "    plt.suptitle(f\"{fileName[0].upper()+fileName[1:]} Confusion Matrix\", fontsize=50)\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.show()\n",
    "    ##printConfusionMatrixToScreen(confusionMatrix)\n",
    "\n",
    "    \n",
    "def calculateAccuracyMetrics(prediction,target):\n",
    "    printmd(f\"**Accuracy:** {metrics.accuracy_score(target,prediction)}\\n\")\n",
    "    printmd(f\"**Precision:** {precision_score(target, prediction,average='weighted',zero_division=1)}\")\n",
    "    printmd(f\"**Recall:** {recall_score(target, prediction,average='weighted')}\")\n",
    "    printmd(f\"**F1 score:** {f1_score(target, prediction,average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Appropriate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printImage(dataImage,letterArray,index):\n",
    "    if(len(dataImage.shape)<=2):\n",
    "        overall=[]\n",
    "        temp = []\n",
    "        counter = 0\n",
    "        for x in range(len(dataImage[index])):\n",
    "            temp.append(dataImage[index][x])\n",
    "            counter=counter+1\n",
    "            if(counter==28):\n",
    "                overall.append(temp)\n",
    "                counter=0\n",
    "                temp=[]\n",
    "            \n",
    "        overall=np.asarray(overall)\n",
    "        plt.imshow(overall, cmap='Greys', vmin=0, vmax=255)\n",
    "        plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "        plt.show()\n",
    "        print(f\"Letter: {letterArray[index]}\")\n",
    "        return\n",
    "    \n",
    "    plt.imshow(dataImage[index], cmap='as_gray', vmin=0, vmax=255)\n",
    "    plt.suptitle(f\"Letter: {letterArray[index]}\", fontsize=20)\n",
    "    plt.show()\n",
    "            \n",
    "# lettersTestTarget=[]\n",
    "# lettersTestArray=generateTestingDataArray()\n",
    "\n",
    "# printImage(letterArray[0])\n",
    "# print(len(lettersTestTarget[0]))\n",
    "# printImage(lettersTestArray[0],lettersTestTarget[0],26)\n",
    "# print(lettersTestTarget[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "\n",
    "def countHowMany1s(arr):\n",
    "    counter = 0\n",
    "    for x in arr:\n",
    "        if(x==1):\n",
    "            counter=counter+1\n",
    "    return counter\n",
    "\n",
    "def testAgainstDictionary():\n",
    "    spell = Speller(lang='en')\n",
    "    identifiedCorrectlyDic=[] ##1 is yes otherwise\n",
    "    identifiedCorrectlyNoDic=[] ##1 is yes otherwise\n",
    "    textSplit=targetText.split(\" \")\n",
    "    wordCounterNoDic = 0\n",
    "    wordCounterWithDic = 0\n",
    "    \n",
    "    for wordIndex in range(len(targetImage)):\n",
    "        characters = np.asarray(targetImage[wordIndex])\n",
    "        characters=characters.reshape(characters.shape[0], 28, 28, 1).astype('float32')\n",
    "        characters = characters/255\n",
    "        prediction = model.predict(characters)\n",
    "        prediction = np.argmax(prediction, axis=1)\n",
    "        prediction = letterConverter(prediction,\"Letters\")\n",
    "        ##We check the correct values without dictionary\n",
    "        if(\"\".join(prediction).lower()==textSplit[wordIndex].lower()):\n",
    "            wordCounterNoDic= wordCounterNoDic +1\n",
    "        counter = 0\n",
    "        while counter<len(prediction) and counter<len(textSplit[wordIndex]):\n",
    "            if(prediction[counter].lower()==textSplit[wordIndex][counter].lower()):\n",
    "                identifiedCorrectlyNoDic.append(1)\n",
    "            else:\n",
    "                identifiedCorrectlyNoDic.append(0)\n",
    "            counter=counter+1\n",
    "        ##We now check against the dictionary corrected values\n",
    "        dictionaryCorrection = list(spell(\"\".join(prediction)))\n",
    "        if(\"\".join(dictionaryCorrection).lower()==textSplit[wordIndex].lower()):\n",
    "            wordCounterWithDic= wordCounterWithDic +1\n",
    "        counter = 0\n",
    "        while counter<len(dictionaryCorrection) and counter<len(textSplit[wordIndex]):\n",
    "            if(dictionaryCorrection[counter].lower()==textSplit[wordIndex][counter].lower()):\n",
    "                identifiedCorrectlyDic.append(1)\n",
    "            else:\n",
    "                identifiedCorrectlyDic.append(0)\n",
    "            counter=counter+1\n",
    "    print(\"No dictionary correction we got \",countHowMany1s(identifiedCorrectlyNoDic),\" Correct\")\n",
    "    print(\"With dictionary we got \",countHowMany1s(identifiedCorrectlyDic))\n",
    "    print(\"Words Dic\",wordCounterWithDic,\" No Words Dic\",wordCounterNoDic)\n",
    "    print(\"Length of array:\",len(identifiedCorrectlyDic),len(identifiedCorrectlyNoDic))\n",
    "    return (np.asarray(identifiedCorrectlyDic),np.asarray(identifiedCorrectlyNoDic))\n",
    "        \n",
    "# print(trainLetter.shape)\n",
    "# temp=np.asarray(targetImage[0])\n",
    "# temp=temp.reshape(len(targetImage[0]), 28, 28, 1).astype('float32')\n",
    "# temp =temp/255\n",
    "# prediction = model.predict(trainLetter)\n",
    "# predictions = np.argmax(prediction, axis=1)\n",
    "# print(predictions)\n",
    "print(\"Words in text:\",len(targetText.split(\" \")))\n",
    "print(\"Words in image:\",len(targetImage))\n",
    "noDictionary,dictionary=testAgainstDictionary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
